{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saqlain2204/GenAI-NLP-Resources/blob/main/LLAMA_3_Fine_tuning_%F0%9F%9B%A0%EF%B8%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qQgNBjUqEyW"
      },
      "source": [
        "<!-- Banner Image -->\n",
        "<img src=\"https://i.ibb.co/NWMSWGF/Blue-Modern-Lets-Do-This-Linked-In-Banner-1.png\" width=\"100%\">\n",
        "\n",
        "<!-- Links -->\n",
        "<center>\n",
        "  <a href=\"https://docs.inferless.com/\" style=\"color: #B8FF33;\">Docs</a> ‚Ä¢\n",
        "  <a href=\"https://tutorials.inferless.com/\" style=\"color: #B8FF33;\">Tutorials</a> ‚Ä¢\n",
        "  <a href=\"https://0ooatrmbp25.typeform.com/to/nzuhQtba?typeform-source=www.inferless.com\" style=\"color: #06b6d4;\">Join Private Beta</a>\n",
        "</center>\n",
        "\n",
        "# Finetune and Inference Llama 3 üõ†Ô∏è\n",
        "\n",
        "Welcome to the Tutorial! üöÄ\n",
        "\n",
        "Let's dive into the supervised fine-tuning with Llama 3 8B model, released by Meta in April 2024. üìö Llama 3 models were trained on 8x more data on over 15 trillion tokens. It has a context length of 8K tokens and increases the vocabulary size of the tokenizer to tokenizer to 128,256 (from 32K tokens in the previous version).\n",
        "\n",
        "üåü In this notebook we will finetune LLama 3 8B model, fine-tuning is the process of taking a pre-trained large language model (LLM) and further training it on a smaller, domain-specific dataset to improve its performance on specific tasks or in certain domains.\n",
        "\n",
        "### Let's get started! üåà\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWMwP5qdqEyY"
      },
      "source": [
        "### Install the required *Libraries*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJrcUCKs7X9S"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U datasets\n",
        "!pip install -q -U trl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfoC4yqrqEyZ"
      },
      "source": [
        "<h4>We will utilize Weights & Biases for metric tracking. Please provide your API key as input.</h4>\n",
        "<h4>Additionally, ensure you include your Hugging Face token, as it is necessary when uploading the model to the Hugging Face Hub.</h4>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0fjz6ldqEyZ"
      },
      "outputs": [],
      "source": [
        "#Login to wandb\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
        "new_model = \"inferless-Llama-3-8B\"\n",
        "#Copy and paste your hf_token\n",
        "hf_token=\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aREsCz3Tu49W"
      },
      "source": [
        "### Import and load all the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te74DElZ8r5t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from trl import SFTTrainer,setup_chat_format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beis_jEvqEya"
      },
      "source": [
        "### Tokenizer and Model\n",
        "Load and initialize the tokenizer with Hugging Face Transformers `AutoTokenizer`.\n",
        "\n",
        "For `ChatML` support we will use `setup_chat_format()` function in `trl` . It will set up the `chat_template` of the tokenizer, adds special tokens to the `tokenizer` and  resizes the model‚Äôs embedding layer to accommodate the new tokens.\n",
        "\n",
        "We will define the `BitsAndBytes` configurations and load the model in the 4-bit precision.\n",
        "\n",
        " Prepare the model for QLoRA training using the `prepare_model_for_kbit_training()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lChdRaiR81Dc"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name,token=hf_token)\n",
        "compute_dtype = getattr(torch, \"float16\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "          model_name, quantization_config=bnb_config, device_map={\"\": 0},token=hf_token)\n",
        "\n",
        "model, tokenizer = setup_chat_format(model, tokenizer)\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1mJrE6mqEya"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Format the dataset\n",
        "Load and preprocess the `HuggingFaceH4/ultrachat_200k` dataset which is a filtered version of the UltraChat dataset from Huggingface.\n",
        "\n",
        "Format the conversation using the `ChatML` template conversational style finetuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3iW0qLAqEya"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"HuggingFaceH4/ultrachat_200k\"\n",
        "dataset = load_dataset(dataset_name, split=\"train_sft\")\n",
        "dataset = dataset.shuffle(seed=42).select(range(10000))\n",
        "\n",
        "def format_chat_template(row):\n",
        "    chat = tokenizer.apply_chat_template(row[\"messages\"], tokenize=False)\n",
        "    return {\"text\":chat}\n",
        "\n",
        "processed_dataset = dataset.map(\n",
        "    format_chat_template,\n",
        "    num_proc= os.cpu_count(),\n",
        ")\n",
        "\n",
        "dataset = processed_dataset.train_test_split(test_size=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCmrG2CEY2tt"
      },
      "source": [
        "### Training Configurations\n",
        "\n",
        "Define the `LoRA Configuration` and the `Training Arguments` which will be used in the TRL's `SFTTrainer`. The SFTTrainer is then created and used to start the fine-tuning process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxc8sr_4-7DM"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "        lora_alpha=64,\n",
        "        lora_dropout=0.05,\n",
        "        r=16,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
        ")\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "        output_dir=\"./results_llama3_sft/\",\n",
        "        evaluation_strategy=\"steps\",\n",
        "        do_eval=True,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        per_device_train_batch_size=8,\n",
        "        gradient_accumulation_steps=2,\n",
        "        per_device_eval_batch_size=8,\n",
        "        log_level=\"debug\",\n",
        "        save_steps=50,\n",
        "        logging_steps=50,\n",
        "        learning_rate=8e-6,\n",
        "        eval_steps=10,\n",
        "        # max_steps=None,\n",
        "        num_train_epochs=1,\n",
        "        warmup_steps=30,\n",
        "        lr_scheduler_type=\"linear\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFnFTZkIZGmg"
      },
      "source": [
        "### üöÄ Start the Training Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bu-_d4YP_CkR"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=dataset['train'],\n",
        "        eval_dataset=dataset['test'],\n",
        "        peft_config=peft_config,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=2024,\n",
        "        tokenizer=tokenizer,\n",
        "        args=training_arguments,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_4IfCSlqEyb"
      },
      "source": [
        "### Save the final checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZyK8gwoqEyb"
      },
      "outputs": [],
      "source": [
        "trainer.model.save_pretrained(\"final_checkpoint\")\n",
        "tokenizer.save_pretrained(\"final_checkpoint\")\n",
        "\n",
        "#load the base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    # return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "\n",
        "#Merge the base model with the \"final_checkpoint\" adapter\n",
        "model = PeftModel.from_pretrained(base_model, \"final_checkpoint\")\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "#Save model and tokenizer\n",
        "model.save_pretrained(new_model)\n",
        "tokenizer.save_pretrained(new_model)\n",
        "\n",
        "#Push them to the HF Hub\n",
        "model.push_to_hub(new_model, use_temp_dir=False, token=hf_token)\n",
        "tokenizer.push_to_hub(new_model, use_temp_dir=False, token=hf_token)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}